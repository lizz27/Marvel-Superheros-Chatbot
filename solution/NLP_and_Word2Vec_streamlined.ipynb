{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Natural Language Processing using NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Install NLTK - pip install nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "NLP Part 0 - Get some Data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import urllib\n",
    "import bs4 as bs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# We will read the contents of the Wikipedia article \"Global_warming\" as an example, please feel free to use your own!\n",
    "# We can open the page using \"urllib.request.urlopen\" then read it using \".read()\"\n",
    "source = urllib.request.urlopen('https://en.wikipedia.org/wiki/Global_warming').read()\n",
    "\n",
    "# Beautiful Soup is a Python library for pulling data out of HTML and XML files.\n",
    "# you may need to install a parser library --> \"!pip3 install lxml\"\n",
    "# Parsing the data/creating BeautifulSoup object\n",
    "\n",
    "soup = bs.BeautifulSoup(source,\"html.parser\") \n",
    "\n",
    "# Fetching the data\n",
    "text = \"\"\n",
    "for paragraph in soup.find_all('p'): #The <p> tag defines a paragraph in the webpages\n",
    "    text += paragraph.text\n",
    "\n",
    "# Preprocessing the data\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',text) # [0-9]* --> Matches zero or more repetitions of any digit from 0 to 9\n",
    "text = text.lower() #everything to lowercase\n",
    "text = re.sub(r'\\W^.?!',' ',text) # \\W --> Matches any character which is not a word character except (.?!)\n",
    "text = re.sub(r'\\d',' ',text) # \\d --> Matches any decimal digit\n",
    "text = re.sub(r'\\s+',' ',text) # \\s --> Matches any characters that are considered whitespace (Ex: [\\t\\n\\r\\f\\v].)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "NLP Part 1 - Tokenization of paragraphs/sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "\n",
    "# Tokenizing sentences\n",
    "sentences = nltk.sent_tokenize(text) #tokenizing or splitting a string, text into a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "\n",
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Tokenizing words\n",
    "all_words = nltk.word_tokenize(text) #tokenizing or splitting a string, text into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print(all_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "NLP Part 2 - Stopwords and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Removing stopwords\n",
    "def remove_stopwords(sentences):\n",
    "    for i in range(len(sentences)):\n",
    "        words = nltk.word_tokenize(sentences[i])\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "        sentences[i] = ' '.join(words)\n",
    "    return sentences\n",
    "sentences = remove_stopwords(sentences)\n",
    "print(sentences[:10]) #eliminating all stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def remove_punctuation(sentences):\n",
    "    for i in range(len(sentences)):\n",
    "        words = nltk.word_tokenize(sentences[i])\n",
    "        words = [word for word in words if word not in \",.?!()\"]\n",
    "        sentences[i] = ' '.join(words)\n",
    "    return sentences\n",
    "sentences = remove_punctuation(sentences)\n",
    "print(sentences[:10]) #eliminating all punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "NLP Part 3a - Stemming the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# try each of the word below\n",
    "stemmer.stem('troubled')\n",
    "#stemmer.stem('trouble')\n",
    "#stemmer.stem('troubling')\n",
    "#stemmer.stem('troubles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "sentences = remove_stopwords(sentences)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stemming\n",
    "def stem_sentences(sentences):\n",
    "    for i in range(len(sentences)):\n",
    "        words = nltk.word_tokenize(sentences[i])\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        sentences[i] = ' '.join(words)\n",
    "    return sentences\n",
    "\n",
    "stemmed_sentences = stem_sentences(sentences)\n",
    "print(stemmed_sentences[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "NLP Part 3b - Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "##Unstem our sentences\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "sentences = remove_stopwords(sentences)\n",
    "sentences = remove_punctuation(sentences)\n",
    "\n",
    "# Lemmatization\n",
    "def lem_sentences(sentences):\n",
    "    for i in range(len(sentences)):\n",
    "        words = nltk.word_tokenize(sentences[i])\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        sentences[i] = ' '.join(words)   \n",
    "    return sentences\n",
    "sentences = lem_sentences(sentences)\n",
    "print(sentences[:10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "NLP Part 4 - POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# POS Tagging\n",
    "\n",
    "# example\n",
    "# CC - coordinating conjunction\n",
    "# NN - noun, singular (cat, tree)\n",
    "#all_words = nltk.word_tokenize(text)  ###If we want to look at part of speech taking before we stem/lem\n",
    "tagged_words = nltk.pos_tag(all_words)\n",
    "##Creates a list of lists where each element of the list is [word,partofspeech abbreviation]\n",
    "# Tagged word paragraph\n",
    "word_tags = []\n",
    "for tw in tagged_words:\n",
    "    word_tags.append(tw[0]+\"_\"+tw[1])\n",
    "\n",
    "tagged_paragraph = ' '.join(word_tags)\n",
    "print(tagged_paragraph[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "nlp_env",
   "resource_dir": "/projects/1a445db5-1004-43af-b767-bfadf704036b/.local/share/jupyter/kernels/nlp_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}