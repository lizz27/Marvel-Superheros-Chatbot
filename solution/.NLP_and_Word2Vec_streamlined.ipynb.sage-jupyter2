{"backend_state":"ready","kernel":"nlp_env","trust":true,"type":"settings"}
{"cell_type":"markdown","id":"317255","input":"NLP Part 4 - POS Tagging","pos":12.5,"type":"cell"}
{"cell_type":"markdown","id":"6cf19d","input":"NLP Part 1 - Tokenization of paragraphs/sentences","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"766bc1","input":"Natural Language Processing using NLTK\n","pos":0,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"788957","input":"NLP Part 3b - Lemmatization","pos":11.96875,"type":"cell"}
{"cell_type":"markdown","id":"ab8381","input":"NLP Part 0 - Get some Data!\n","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"bea3f0","input":"NLP Part 3a - Stemming the words","pos":11.875,"type":"cell"}
{"cell_type":"markdown","id":"fe2ab3","input":"NLP Part 2 - Stopwords and Punctuation","pos":10,"type":"cell"}
{"id":"02bbd0","input":"# We will read the contents of the Wikipedia article \"Global_warming\" as an example, please feel free to use your own!\n# We can open the page using \"urllib.request.urlopen\" then read it using \".read()\"\nsource = urllib.request.urlopen('https://en.wikipedia.org/wiki/Global_warming').read()\n\n# Beautiful Soup is a Python library for pulling data out of HTML and XML files.\n# you may need to install a parser library --> \"!pip3 install lxml\"\n# Parsing the data/creating BeautifulSoup object\n\nsoup = bs.BeautifulSoup(source,\"html.parser\") \n\n# Fetching the data\ntext = \"\"\nfor paragraph in soup.find_all('p'): #The <p> tag defines a paragraph in the webpages\n    text += paragraph.text\n\n# Preprocessing the data\ntext = re.sub(r'\\[[0-9]*\\]',' ',text) # [0-9]* --> Matches zero or more repetitions of any digit from 0 to 9\ntext = text.lower() #everything to lowercase\ntext = re.sub(r'\\W^.?!',' ',text) # \\W --> Matches any character which is not a word character except (.?!)\ntext = re.sub(r'\\d',' ',text) # \\d --> Matches any decimal digit\ntext = re.sub(r'\\s+',' ',text) # \\s --> Matches any characters that are considered whitespace (Ex: [\\t\\n\\r\\f\\v].)","pos":4,"type":"cell"}
{"id":"20a908","input":"from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n##Unstem our sentences\nsentences = nltk.sent_tokenize(text)\nsentences = remove_stopwords(sentences)\nsentences = remove_punctuation(sentences)\n\n# Lemmatization\ndef lem_sentences(sentences):\n    for i in range(len(sentences)):\n        words = nltk.word_tokenize(sentences[i])\n        words = [lemmatizer.lemmatize(word) for word in words]\n        sentences[i] = ' '.join(words)   \n    return sentences\nsentences = lem_sentences(sentences)\nprint(sentences[:10]) ","pos":12,"type":"cell"}
{"id":"221e86","input":"text[:100]","pos":5,"type":"cell"}
{"id":"440a90","input":"import urllib\nimport bs4 as bs\nimport re","pos":3,"type":"cell"}
{"id":"4c6c88","input":"","pos":12.9375,"type":"cell"}
{"id":"58acf2","input":"# Install NLTK - pip install nltk\nimport nltk\nnltk.download('wordnet')\nnltk.download('punkt')","pos":1,"type":"cell"}
{"id":"5d0500","input":"print(all_words[:10])","pos":9,"type":"cell"}
{"id":"762f0e","input":"","pos":12.96875,"type":"cell"}
{"id":"98ec2d","input":"# Removing stopwords\ndef remove_stopwords(sentences):\n    for i in range(len(sentences)):\n        words = nltk.word_tokenize(sentences[i])\n        words = [word for word in words if word not in stopwords.words('english')]\n        sentences[i] = ' '.join(words)\n    return sentences\nsentences = remove_stopwords(sentences)\nprint(sentences[:10]) #eliminating all stop words","pos":11.5,"type":"cell"}
{"id":"aaf5a8","input":"\n# Tokenizing sentences\nsentences = nltk.sent_tokenize(text) #tokenizing or splitting a string, text into a list of sentences.","pos":7,"type":"cell"}
{"id":"b9b23a","input":"","pos":12.984375,"type":"cell"}
{"id":"bcb351","input":"nltk.download('stopwords')\nfrom nltk.corpus import stopwords","pos":11,"type":"cell"}
{"id":"d08066","input":"\nprint(sentences[:10])","pos":7.5,"type":"cell"}
{"id":"d1db72","input":"","pos":13,"type":"cell"}
{"id":"d2c13a","input":"sentences = nltk.sent_tokenize(text)\nsentences = remove_stopwords(sentences)\nstemmer = PorterStemmer()\n\n# Stemming\ndef stem_sentences(sentences):\n    for i in range(len(sentences)):\n        words = nltk.word_tokenize(sentences[i])\n        words = [stemmer.stem(word) for word in words]\n        sentences[i] = ' '.join(words)\n    return sentences\n\nstemmed_sentences = stem_sentences(sentences)\nprint(stemmed_sentences[:10])","pos":11.953125,"type":"cell"}
{"id":"e5e191","input":"nltk.download('averaged_perceptron_tagger')","pos":12.75,"type":"cell"}
{"id":"e7465b","input":"# Tokenizing words\nall_words = nltk.word_tokenize(text) #tokenizing or splitting a string, text into a list of words.","pos":8,"type":"cell"}
{"id":"e8b363","input":"def remove_punctuation(sentences):\n    for i in range(len(sentences)):\n        words = nltk.word_tokenize(sentences[i])\n        words = [word for word in words if word not in \",.?!()\"]\n        sentences[i] = ' '.join(words)\n    return sentences\nsentences = remove_punctuation(sentences)\nprint(sentences[:10]) #eliminating all punctuation","pos":11.75,"type":"cell"}
{"id":"e90036","input":"# POS Tagging\n\n# example\n# CC - coordinating conjunction\n# NN - noun, singular (cat, tree)\n#all_words = nltk.word_tokenize(text)  ###If we want to look at part of speech taking before we stem/lem\ntagged_words = nltk.pos_tag(all_words)\n##Creates a list of lists where each element of the list is [word,partofspeech abbreviation]\n# Tagged word paragraph\nword_tags = []\nfor tw in tagged_words:\n    word_tags.append(tw[0]+\"_\"+tw[1])\n\ntagged_paragraph = ' '.join(word_tags)\nprint(tagged_paragraph[:1000])","pos":12.875,"type":"cell"}
{"id":"e9aea2","input":"from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\n# try each of the word below\nstemmer.stem('troubled')\n#stemmer.stem('trouble')\n#stemmer.stem('troubling')\n#stemmer.stem('troubles')","pos":11.9375,"type":"cell"}
{"last_load":1654869299431,"type":"file"}